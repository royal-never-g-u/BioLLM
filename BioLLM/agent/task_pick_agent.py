#!/usr/bin/env python3
"""
Task Pick Agent - Analyzes user input and matches it to analysis types
"""

import os
import sys
import json
from typing import Dict, Any, Optional, List, Tuple

# Add the project root to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from bio_task import update_current_task, get_current_task
from analysis_types import ANALYSIS_TYPES, TASK_PROMPT

class TaskPickAgent:
    """
    Agent that analyzes user input and matches it to analysis types
    """
    
    def __init__(self):
        """Initialize the TaskPickAgent"""
        self.analysis_types = ANALYSIS_TYPES
        self.task_prompt = TASK_PROMPT
        
    def analyze_user_input(self, user_input: str) -> Dict[str, Any]:
        """
        Analyze user input and determine if it matches any analysis type
        
        Args:
            user_input (str): User's input text
            
        Returns:
            Dict containing analysis results and matched task type
        """
        try:
            print(f"ğŸ” TaskPickAgent: Analyzing user input: '{user_input}'")
            
            # Create LLM prompt for analysis
            llm_prompt = self._create_analysis_prompt(user_input)
            
            # Simulate LLM analysis (in real implementation, this would call an actual LLM)
            analysis_result = self._simulate_llm_analysis(llm_prompt, user_input)
            
            # Process the analysis result
            matched_task_type = self._process_analysis_result(analysis_result, user_input)
            
            # Update bio_task only if a valid match is found
            if matched_task_type is not None:
                self._update_bio_task(matched_task_type)
                
                # Check if we should trigger if_next_step
                next_step_result = self._check_and_trigger_next_step()
                
                return {
                    'success': True,
                    'user_input': user_input,
                    'matched_task_type': matched_task_type,
                    'analysis_result': analysis_result,
                    'bio_task_updated': True,
                    'next_step_triggered': next_step_result is not None,
                    'next_step_result': next_step_result
                }
            else:
                # Don't update bio_task if no match found, preserve existing task_type
                print(f"â„¹ï¸ TaskPickAgent: æœªåŒ¹é…åˆ°åˆ†æç±»å‹ï¼Œä¿æŒç°æœ‰task_typeä¸å˜")
                
                return {
                    'success': True,
                    'user_input': user_input,
                    'matched_task_type': matched_task_type,
                    'analysis_result': analysis_result,
                    'bio_task_updated': False,
                    'next_step_triggered': False,
                    'next_step_result': None
                }
            
        except Exception as e:
            print(f"âŒ Error in TaskPickAgent: {e}")
            return {
                'success': False,
                'error': str(e),
                'user_input': user_input,
                'matched_task_type': None,
                'bio_task_updated': False
            }
    
    def _create_analysis_prompt(self, user_input: str) -> str:
        """
        Create LLM prompt for analyzing user input
        
        Args:
            user_input (str): User's input text
            
        Returns:
            str: Formatted prompt for LLM analysis
        """
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·è¾“å…¥ï¼Œåˆ¤æ–­å®ƒæ˜¯å¦åŒ¹é…ä»¥ä¸‹åˆ†æç±»å‹ä¸­çš„æŸä¸€ä¸ªã€‚

å¯ç”¨çš„åˆ†æç±»å‹ï¼š
{self._format_analysis_types()}

ä»»åŠ¡è¯´æ˜ï¼š
{self.task_prompt}

ç”¨æˆ·è¾“å…¥ï¼š"{user_input}"

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š

1. æ˜¯å¦åŒ¹é…ï¼šæ˜¯/å¦
2. åŒ¹é…çš„åˆ†æç±»å‹IDï¼ˆå¦‚æœåŒ¹é…ï¼‰ï¼š1-6çš„æ•°å­—
3. åŒ¹é…çš„åˆ†æç±»å‹åç§°ï¼ˆå¦‚æœåŒ¹é…ï¼‰ï¼šå®Œæ•´çš„åˆ†æç±»å‹åç§°
4. åŒ¹é…ç†ç”±ï¼šç®€è¦è¯´æ˜ä¸ºä»€ä¹ˆåŒ¹é…æˆ–ä¸åŒ¹é…
5. ç½®ä¿¡åº¦ï¼šé«˜/ä¸­/ä½

ç¤ºä¾‹å›ç­”ï¼š
1. æ˜¯å¦åŒ¹é…ï¼šæ˜¯
2. åŒ¹é…çš„åˆ†æç±»å‹IDï¼š2
3. åŒ¹é…çš„åˆ†æç±»å‹åç§°ï¼šGene Knockout Analysis
4. åŒ¹é…ç†ç”±ï¼šç”¨æˆ·æåˆ°äº†"åŸºå› æ•²é™¤"å’Œ"åˆ é™¤åŸºå› "ï¼Œè¿™ä¸åŸºå› æ•²é™¤åˆ†æç›´æ¥ç›¸å…³
5. ç½®ä¿¡åº¦ï¼šé«˜

è¯·åˆ†æä¸Šè¿°ç”¨æˆ·è¾“å…¥ï¼š
"""
        return prompt
    
    def _format_analysis_types(self) -> str:
        """
        Format analysis types for display
        
        Returns:
            str: Formatted analysis types string
        """
        formatted = ""
        for key, value in self.analysis_types.items():
            formatted += f"{key}. {value}\n"
        return formatted
    
    def _simulate_llm_analysis(self, prompt: str, user_input: str) -> Dict[str, Any]:
        """
        Simulate LLM analysis (in real implementation, this would call an actual LLM)
        
        Args:
            prompt (str): LLM prompt
            user_input (str): User input for context
            
        Returns:
            Dict containing simulated LLM response
        """
        # This is a simulation - in real implementation, you would call an actual LLM
        # For now, we'll use a simple keyword matching approach
        
        user_input_lower = user_input.lower()
        
        # Define keywords for each analysis type
        keywords = {
            1: ['flux balance', 'fba', 'é€šé‡å¹³è¡¡', 'ä»£è°¢é€šé‡', 'flux', 'balance', '1'],
            2: ['gene knockout', 'gene deletion', 'åŸºå› æ•²é™¤', 'åŸºå› åˆ é™¤', 'knockout', 'deletion', '2'],
            3: ['phenotype', 'è¡¨å‹', 'phenotype prediction', 'è¡¨å‹é¢„æµ‹', '3'],
            4: ['pathway', 'é€”å¾„', 'pathway analysis', 'é€”å¾„åˆ†æ', '4'],
            5: ['evolutionary', 'è¿›åŒ–', 'evolution', 'evolutionary analysis', 'è¿›åŒ–åˆ†æ', '5'],
            6: ['constraint', 'çº¦æŸ', 'constraint-based', 'çº¦æŸåŸºç¡€', '6']
        }
        
        # Find matches
        matches = []
        for analysis_id, keyword_list in keywords.items():
            for keyword in keyword_list:
                if keyword in user_input_lower:
                    matches.append({
                        'analysis_id': analysis_id,
                        'analysis_name': self.analysis_types[analysis_id],
                        'matched_keyword': keyword,
                        'confidence': 'high' if len(keyword) > 3 else 'medium'
                    })
                    break  # Only match once per analysis type
        
        if matches:
            # Return the best match (first one found)
            best_match = matches[0]
            return {
                'is_match': True,
                'analysis_id': best_match['analysis_id'],
                'analysis_name': best_match['analysis_name'],
                'matched_keyword': best_match['matched_keyword'],
                'confidence': best_match['confidence'],
                'reason': f"ç”¨æˆ·è¾“å…¥åŒ…å«å…³é”®è¯ '{best_match['matched_keyword']}'ï¼ŒåŒ¹é…åˆ†æç±»å‹ '{best_match['analysis_name']}'"
            }
        else:
            return {
                'is_match': False,
                'analysis_id': None,
                'analysis_name': None,
                'matched_keyword': None,
                'confidence': 'low',
                'reason': "ç”¨æˆ·è¾“å…¥ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸åˆ†æç±»å‹ç›¸å…³çš„å…³é”®è¯"
            }
    
    def _process_analysis_result(self, analysis_result: Dict[str, Any], user_input: str) -> Optional[int]:
        """
        Process LLM analysis result and extract matched task type
        
        Args:
            analysis_result (Dict): LLM analysis result
            user_input (str): Original user input
            
        Returns:
            Optional[int]: Matched task type ID or None
        """
        if analysis_result.get('is_match', False):
            analysis_id = analysis_result.get('analysis_id')
            analysis_name = analysis_result.get('analysis_name')
            confidence = analysis_result.get('confidence', 'low')
            
            print(f"âœ… TaskPickAgent: åŒ¹é…åˆ°åˆ†æç±»å‹ {analysis_id} - {analysis_name}")
            print(f"   ç½®ä¿¡åº¦: {confidence}")
            print(f"   åŒ¹é…ç†ç”±: {analysis_result.get('reason', 'N/A')}")
            
            return analysis_id
        else:
            print(f"âŒ TaskPickAgent: æœªåŒ¹é…åˆ°ä»»ä½•åˆ†æç±»å‹")
            print(f"   åŸå› : {analysis_result.get('reason', 'N/A')}")
            return None
    
    def _update_bio_task(self, task_type: int) -> bool:
        """
        Update bio_task with the matched task type
        
        Args:
            task_type (int): Task type ID to set
            
        Returns:
            bool: True if update was successful
        """
        try:
            # Get current task to preserve other fields
            current_task = get_current_task()
            
            # Update task type
            success = update_current_task(task_type=task_type)
            
            if success:
                print(f"âœ… TaskPickAgent: æˆåŠŸæ›´æ–°bio_taskï¼Œè®¾ç½®task_typeä¸º {task_type}")
                print(f"   åˆ†æç±»å‹: {self.analysis_types.get(task_type, 'Unknown')}")
            else:
                print(f"âŒ TaskPickAgent: æ›´æ–°bio_taskå¤±è´¥")
            
            return success
            
        except Exception as e:
            print(f"âŒ TaskPickAgent: æ›´æ–°bio_taskæ—¶å‡ºé”™: {e}")
            return False
    
    def get_available_analysis_types(self) -> Dict[int, str]:
        """
        Get all available analysis types
        
        Returns:
            Dict[int, str]: Dictionary of available analysis types
        """
        return self.analysis_types.copy()
    
    def validate_task_type(self, task_type: int) -> bool:
        """
        Validate if a task type is valid
        
        Args:
            task_type (int): Task type to validate
            
        Returns:
            bool: True if valid, False otherwise
        """
        return task_type in self.analysis_types
    
    def _check_and_trigger_next_step(self) -> Optional[Dict[str, Any]]:
        """
        Check if bio_task has both model_name and task_type, and trigger if_next_step if so
        
        Returns:
            Optional[Dict[str, Any]]: Next step result if triggered, None otherwise
        """
        try:
            # Get current bio_task
            current_task = get_current_task()
            
            # Check if both model_name and task_type exist
            has_model_name = bool(current_task.model_name and current_task.model_name.strip())
            has_task_type = current_task.task_type is not None and current_task.task_type != ""
            
            if has_model_name and has_task_type:
                print(f"ğŸ¯ TaskPickAgent: æ£€æµ‹åˆ°å®Œæ•´é…ç½® (model_name: {current_task.model_name}, task_type: {current_task.task_type})")
                print(f"ğŸ”„ è‡ªåŠ¨è§¦å‘ if_next_step...")
                
                # Import and call if_next_step
                from agent.next_step_agent import if_next_step
                next_step_result = if_next_step()
                
                print(f"âœ… TaskPickAgent: if_next_step æ‰§è¡Œå®Œæˆ")
                print(f"   ç»“æœ: {next_step_result.get('action', 'unknown')}")
                
                return next_step_result
            else:
                print(f"â„¹ï¸ TaskPickAgent: é…ç½®ä¸å®Œæ•´ï¼Œä¸è§¦å‘ if_next_step")
                print(f"   model_name: {'æœ‰' if has_model_name else 'æ— '}")
                print(f"   task_type: {'æœ‰' if has_task_type else 'æ— '}")
                return None
                
        except Exception as e:
            print(f"âŒ TaskPickAgent: æ£€æŸ¥å¹¶è§¦å‘ if_next_step æ—¶å‡ºé”™: {e}")
            return None


def analyze_user_input_for_task_type(user_input: str) -> Dict[str, Any]:
    """
    Global function to analyze user input and match to task type
    
    Args:
        user_input (str): User's input text
        
    Returns:
        Dict containing analysis results
    """
    agent = TaskPickAgent()
    return agent.analyze_user_input(user_input)


def get_task_pick_agent() -> TaskPickAgent:
    """
    Get a TaskPickAgent instance
    
    Returns:
        TaskPickAgent: TaskPickAgent instance
    """
    return TaskPickAgent()


# Example usage and testing
if __name__ == "__main__":
    # Test the TaskPickAgent
    agent = TaskPickAgent()
    
    # Test cases
    test_inputs = [
        "æˆ‘æƒ³è¿›è¡ŒåŸºå› æ•²é™¤åˆ†æ",
        "è¯·å¸®æˆ‘åšé€šé‡å¹³è¡¡åˆ†æ",
        "éœ€è¦è¡¨å‹é¢„æµ‹",
        "åˆ†æä»£è°¢é€”å¾„",
        "è¿›è¡Œè¿›åŒ–åˆ†æ",
        "çº¦æŸåŸºç¡€åˆ†æ",
        "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·",  # Should not match
        "åˆ†æåŸºå› è¡¨è¾¾",    # Should not match
    ]
    
    print("ğŸ§¬ TaskPickAgent æµ‹è¯•")
    print("=" * 60)
    
    for i, test_input in enumerate(test_inputs, 1):
        print(f"\nğŸ“‹ æµ‹è¯• {i}: '{test_input}'")
        result = agent.analyze_user_input(test_input)
        
        if result['success']:
            if result['matched_task_type']:
                print(f"   âœ… åŒ¹é…æˆåŠŸ: {result['matched_task_type']}")
                print(f"   ğŸ“ åˆ†æç±»å‹: {agent.analysis_types.get(result['matched_task_type'], 'Unknown')}")
            else:
                print(f"   âŒ æœªåŒ¹é…åˆ°åˆ†æç±»å‹")
            print(f"   ğŸ”„ bio_taskå·²æ›´æ–°: {result['bio_task_updated']}")
        else:
            print(f"   ğŸ’¥ åˆ†æå¤±è´¥: {result.get('error', 'Unknown error')}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")
